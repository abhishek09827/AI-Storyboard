{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Animated Storyboard Generator\n",
        "\n",
        "\n",
        "This Colab notebook creates a web interface using **Gradio** to automatically generate animated storyboards. It integrates several powerful AI models to transform a simple topic into a full video with images and voiceover.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "* **AI-Powered Script & Prompt Generation:** Utilizes **OpenAI's GPT** to generate a complete storyboard script (visuals and narration) from a user-provided topic. It then automatically converts these scene descriptions into optimized prompts for the image generation model.\n",
        "\n",
        "* **High-Quality Image Generation:** Employs **Stable Diffusion XL** (with a base and refiner model) to create visually appealing images corresponding to each scene in the script.\n",
        "\n",
        "* **Realistic Voiceovers:** Integrates with the **Eleven Labs API** to generate high-quality, natural-sounding voiceovers for the narration part of the script.\n",
        "\n",
        "* **Automated Animation:** Uses the **moviepy** library to seamlessly combine the generated images and audio into a final animated video storyboard.\n",
        "\n",
        "* **Interactive Web Interface:** The entire process is wrapped in a user-friendly **Gradio** interface with two main tabs:\n",
        "    * **Generate Storyboard:** Input a topic to create the script and generate all the necessary images.\n",
        "    * **Create Animation:** A single click to compile the generated images and narrations into a final video.\n",
        "\n",
        "### Core Technologies Used:\n",
        "\n",
        "* **Web UI:** Gradio\n",
        "* **Image Generation:** Stable Diffusion XL (via `diffusers`)\n",
        "* **Language Models:** OpenAI GPT, BERT (for text embeddings)\n",
        "* **Audio Generation:** Eleven Labs API\n",
        "* **Video Processing:** moviepy\n",
        "* **Core Libraries:** `transformers`, `accelerate`, `pillow`, `scikit-learn`, `openai`"
      ],
      "metadata": {
        "id": "zrZrZDojSzbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install Dependencies\n",
        "#Installs required libraries using pip. -q suppresses output."
      ],
      "metadata": {
        "id": "mkrQrTIDPMs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOHlj_A0OV3j",
        "outputId": "f81d56f1-c8a3-4c7f-9ce8-0b0d2255938b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m850.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "## 1. Install Dependencies\n",
        "!pip install -q diffusers transformers accelerate scikit-learn pillow huggingface-hub gradio openai==0.28 moviepy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries for the application.\n"
      ],
      "metadata": {
        "id": "Uov5RnzxPSPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-peXjsHXOaDh",
        "outputId": "b0146cb6-c60e-4da8-9c02-b719dc1c9317"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import io\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DiffusionPipeline, EulerDiscreteScheduler, AutoencoderKL\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from moviepy.editor import ImageClip,AudioFileClip,concatenate_videoclips,CompositeAudioClip\n",
        "from moviepy.config import change_settings\n",
        "import requests\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "import json\n",
        "import openai\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constant variables for models and API keys.\n"
      ],
      "metadata": {
        "id": "YURSdaA4PZL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7gxkIjQMsrh7"
      },
      "outputs": [],
      "source": [
        "base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "style_model = \"blink7630/storyboard-sketch\"\n",
        "ELEVENLABS_API_KEY = \"\"\n",
        "VOICE_ID = \"onwK4e9ZLuTAKqWW03F9\"\n",
        "GPT_MODEL = \"gpt-4.1\"\n",
        "GPT_MODEL_2  = \"gpt-4o\"\n",
        "openai_api_key = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and print available GPU information.\n"
      ],
      "metadata": {
        "id": "Y1B2oW_oPcfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zAw-OAsuRcFC",
        "outputId": "1e96842c-4a62-42c2-dc07-2f0ed3e9ebbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Available: True\n",
            "GPU Device: Tesla T4\n",
            "GPU Memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to load Stable Diffusion and BERT models.\n",
        "## 3. Core Functions\n"
      ],
      "metadata": {
        "id": "5XtqXwfMPepq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ADQp29sp6u1G"
      },
      "outputs": [],
      "source": [
        "## 3. Core Functions\n",
        "def load_models(model_id=base_model):\n",
        "    \"\"\"Load models with optional quantization for smaller GPUs\"\"\"\n",
        "\n",
        "    dtype = torch.float16\n",
        "    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\",torch_dtype=torch.float16)\n",
        "    try:\n",
        "        base = DiffusionPipeline.from_pretrained(\n",
        "        base_model,\n",
        "        vae=vae,\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "        )\n",
        "\n",
        "        base.load_lora_weights(style_model)\n",
        "        _ = base.to(\"cuda\")\n",
        "        refiner = DiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "        text_encoder_2=base.text_encoder_2,\n",
        "        vae=base.vae,\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        variant=\"fp16\",\n",
        "        )\n",
        "        _ = refiner.to(\"cuda\")\n",
        "        bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")\n",
        "        return base, refiner, bert_tokenizer, bert_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def bert_sentence_embedding(sentence, tokenizer, model, device):\n",
        "    \"\"\"Get BERT embedding for a sentence\"\"\"\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**{k: v.to(device) for k, v in tokens.items()})\n",
        "    return output.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "def find_similar_prompt(query, string_list, tokenizer, model, device):\n",
        "    \"\"\"Find the most similar prompt using BERT embeddings\"\"\"\n",
        "    if not string_list:\n",
        "        return 0\n",
        "    query_embedding = bert_sentence_embedding(query, tokenizer, model, device)\n",
        "    string_embeddings = [bert_sentence_embedding(s, tokenizer, model, device) for s in string_list]\n",
        "    similarities = [cosine_similarity(query_embedding, s.reshape(1, -1))[0, 0] for s in string_embeddings]\n",
        "    return similarities.index(max(similarities))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to load models globally to avoid reloading.\n"
      ],
      "metadata": {
        "id": "Qe9eHl0bPhLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gPPeXDrYR_2A"
      },
      "outputs": [],
      "source": [
        "# Global variables to store models\n",
        "global_models = None\n",
        "\n",
        "def load_global_models(model_id):\n",
        "    \"\"\"Load models globally to avoid reloading\"\"\"\n",
        "    global global_models\n",
        "    device = \"cuda\"\n",
        "    if global_models is None or global_models[0] != model_id:\n",
        "        base, refiner, bert_tokenizer, bert_model = load_models(model_id)\n",
        "        global_models = (model_id, base, refiner, bert_tokenizer, bert_model)\n",
        "\n",
        "    return global_models[1], global_models[2], global_models[3], global_models[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Gradio Interface Function\n"
      ],
      "metadata": {
        "id": "q0rPqd4BQ-6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VqQAWt1SOlj9"
      },
      "outputs": [],
      "source": [
        "## 4. Gradio Interface Function\n",
        "def generate_storyboard(\n",
        "    prompts,\n",
        "    model_id=base_model,\n",
        "    prefix=\" \",\n",
        "    strength=0.8,\n",
        "    guidance_scale=7.5,\n",
        "    use_similarity=True,\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    prompt_list = [p.strip() for p in prompts.splitlines() if p.strip()]\n",
        "    if not prompt_list:\n",
        "        return None, \"Error: No valid prompts provided\"\n",
        "    device = \"cuda\"\n",
        "    try:\n",
        "        progress(0, desc=\"Loading models...\")\n",
        "        base, refiner, bert_tokenizer, bert_model = load_global_models(model_id)\n",
        "\n",
        "        generated_images = []\n",
        "\n",
        "        for idx, prompt in enumerate(prompt_list):\n",
        "            full_prompt = prefix + prompt\n",
        "            progress_val = 0.05 + 0.9 * (idx / len(prompt_list))\n",
        "            progress(progress_val, desc=f\"Generating image {idx+1}/{len(prompt_list)}: {prompt}\")\n",
        "\n",
        "            # Step 1: Generate latent image from base model\n",
        "            base_output = base(\n",
        "                prompt=full_prompt,\n",
        "                num_inference_steps=50,\n",
        "                guidance_scale=guidance_scale,\n",
        "                output_type=\"latent\",\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            # Step 2: Refine using refiner\n",
        "            refined_output = refiner(\n",
        "                prompt=full_prompt,\n",
        "                num_inference_steps=20,\n",
        "                guidance_scale=guidance_scale,\n",
        "                image=base_output.images,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            final_image = refined_output.images[0]\n",
        "            generated_images.append(final_image)\n",
        "\n",
        "        # Create a grid of images\n",
        "        rows = (len(generated_images) + 2) // 3\n",
        "        grid_height = rows * 512\n",
        "        grid_width = min(3, len(generated_images)) * 512\n",
        "\n",
        "        grid = Image.new('RGB', (grid_width, grid_height))\n",
        "        for i, img in enumerate(generated_images):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "            grid.paste(img, (col * 512, row * 512))\n",
        "\n",
        "        os.makedirs(\"output\", exist_ok=True)\n",
        "        for i, img in enumerate(generated_images):\n",
        "            img.save(f\"output/frame_{i+1}.png\")\n",
        "\n",
        "        with open(\"output/prompts.txt\", \"w\") as f:\n",
        "            for i, prompt in enumerate(prompt_list):\n",
        "                f.write(f\"Frame {i+1}: {prompt}\\n\")\n",
        "\n",
        "        with zipfile.ZipFile(\"output/storyboard.zip\", \"w\") as zipf:\n",
        "            for i in range(len(generated_images)):\n",
        "                zipf.write(f\"output/frame_{i+1}.png\", f\"frame_{i+1}.png\")\n",
        "            zipf.write(\"output/prompts.txt\", \"prompts.txt\")\n",
        "\n",
        "        return grid, \"Storyboard generated successfully! Click the download button to get all images.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error generating storyboard: {e}\"\n",
        "\n",
        "def create_download_btn():\n",
        "    \"\"\"Create download button for the zip file\"\"\"\n",
        "    if os.path.exists(\"output/storyboard.zip\"):\n",
        "        with open(\"output/storyboard.zip\", \"rb\") as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to generate a cute voice from text using Eleven Labs API.\n"
      ],
      "metadata": {
        "id": "K7XcQRLYRDJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HSD_sbFIToBg"
      },
      "outputs": [],
      "source": [
        "def generate_cute_voice(prompt, output_audio_path):\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}\"\n",
        "    headers = {\n",
        "        \"xi-api-key\": ELEVENLABS_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"text\": prompt,\n",
        "        \"model_id\": \"eleven_monolingual_v1\",\n",
        "        \"voice_settings\": {\n",
        "            \"stability\": 0.5,\n",
        "            \"similarity_boost\": 0.75,\n",
        "            \"style\": 0.5,\n",
        "            \"use_speaker_boost\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_audio_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to generate voice: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to create an animated storyboard video with generated voiceovers.\n"
      ],
      "metadata": {
        "id": "qCRkYP7lRG_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bHPSrKfWapkh"
      },
      "outputs": [],
      "source": [
        "def create_storyboard_animation(\n",
        "    images,\n",
        "    prompts,\n",
        "    output_path=\"output/storyboard_animation.mp4\",\n",
        "    fps=24,\n",
        "    gap_between_frames=1.0\n",
        "):\n",
        "    clips = []\n",
        "\n",
        "    for i, (img, prompt) in enumerate(zip(images, prompts)):\n",
        "        print(f\"Generating voice for frame {i+1}...\")\n",
        "\n",
        "        img_path = f\"output/frame_{i}.png\"\n",
        "        img.save(img_path)\n",
        "\n",
        "        audio_path = f\"output/audio_{i}.mp3\"\n",
        "        generate_cute_voice(prompt, audio_path)\n",
        "\n",
        "        audio_clip = AudioFileClip(audio_path)\n",
        "        duration = audio_clip.duration + gap_between_frames\n",
        "\n",
        "        image_clip = (\n",
        "            ImageClip(img_path)\n",
        "            .set_duration(duration)\n",
        "            .set_audio(audio_clip)\n",
        "            .fadein(0.3)\n",
        "            .fadeout(0.3)\n",
        "        )\n",
        "        clips.append(image_clip)\n",
        "\n",
        "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "    print(f\"Exporting final video to: {output_path}\")\n",
        "    final_video.write_videofile(output_path, codec='libx264', audio_codec='aac', fps=fps)\n",
        "\n",
        "    print(\"Video created successfully.\")\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WlStB1wQLaod",
        "outputId": "03821032-2ddd-419d-925d-71b6ef4386fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Act as a storyboard concept artist. Create an animated short for high-school and college students on the theme \"{topic}\".\\n\\nSTORY RULES\\nProduce 8 to 12 scenes.\\n\\nCharacters / Visual Focus:\\n1. Analyze the topic: decide whether it is best explained through human actions (e.g., \"Teamwork\", \"Public Speaking\", \"Garbage Management\") or through non-human elements / abstract visuals (e.g., \"Photosynthesis\", \"Water Cycle\", \"Cloud Formation\").\\n2. Establish the initial visual focus in Scene 1:\\n   • Generic human characters interacting with the topic, or\\n   • Specific non-human elements (plants, animals, objects, environments, diagrams, etc.).\\n3. Maintain consistency: whatever focus you establish in Scene 1 (people-centric or non-human) must remain consistent in every subsequent scene.\\n\\nEnsure logical continuity so each scene flows naturally from the previous one (cause-and-effect, escalating stakes, or step-by-step problem solving).\\nWrite narration only in third person—describe actions and emotions; no direct dialogue.\\nKeep all content age-appropriate, engaging, and relevant to older students.\\n\\nOUTPUT FORMAT\\nReturn only a JSON array—no extra text. Each element must follow this schema:\\n\\n{{\\n  \"scene_number\": <integer>,\\n  \"visual_cue\": \"Brief note on the setting or key visual action\",\\n  \"narration\": \"Third-person description of what is happening and why it matters\"\\n}}\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Act as a storyboard concept artist. Create an animated short for high-school and college students on the theme \"{topic}\".\n",
        "\n",
        "STORY RULES\n",
        "Produce 8 to 12 scenes.\n",
        "\n",
        "Characters / Visual Focus:\n",
        "1. Analyze the topic: decide whether it is best explained through human actions (e.g., \"Teamwork\", \"Public Speaking\", \"Garbage Management\") or through non-human elements / abstract visuals (e.g., \"Photosynthesis\", \"Water Cycle\", \"Cloud Formation\").\n",
        "2. Establish the initial visual focus in Scene 1:\n",
        "   • Generic human characters interacting with the topic, or\n",
        "   • Specific non-human elements (plants, animals, objects, environments, diagrams, etc.).\n",
        "3. Maintain consistency: whatever focus you establish in Scene 1 (people-centric or non-human) must remain consistent in every subsequent scene.\n",
        "\n",
        "Ensure logical continuity so each scene flows naturally from the previous one (cause-and-effect, escalating stakes, or step-by-step problem solving).\n",
        "Write narration only in third person—describe actions and emotions; no direct dialogue.\n",
        "Keep all content age-appropriate, engaging, and relevant to older students.\n",
        "\n",
        "OUTPUT FORMAT\n",
        "Return only a JSON array—no extra text. Each element must follow this schema:\n",
        "\n",
        "{{\n",
        "  \"scene_number\": <integer>,\n",
        "  \"visual_cue\": \"Brief note on the setting or key visual action\",\n",
        "  \"narration\": \"Third-person description of what is happening and why it matters\"\n",
        "}}\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GPT models and API key for generating storyboard scripts.\n"
      ],
      "metadata": {
        "id": "jjUB1ICzRQYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oFnTnOuOdJQs"
      },
      "outputs": [],
      "source": [
        "GPT_MODEL = \"gpt-4.1\"\n",
        "GPT_MODEL_2  = \"gpt-4o\"\n",
        "openai_api_key = \"\"\n",
        "\n",
        "global narrations\n",
        "# Store last scenes for introspection\n",
        "last_scenes = []\n",
        "narrations = []\n",
        "\n",
        "def generate_storyboard_script(topic: str) -> list:\n",
        "    if not topic.strip():\n",
        "        raise ValueError(\"Topic cannot be empty\")\n",
        "    system_msg = (\n",
        "    f'''Act as a storyboard concept artist. Create an animated short for high-school and college students on the theme: \"{topic}\".\n",
        "\n",
        "STORY RULES\n",
        "\n",
        "- Produce between ** 3 scenes**.\n",
        "- Dynamically adapt the storytelling style:\n",
        "  - If the topic is **non-technical** (e.g., \"Teamwork\", \"Peer Pressure\", \"Procrastination\"), use a **character-driven** approach with relatable human figures in everyday settings.\n",
        "  - If the topic is **technical or scientific** (e.g., \"Climate Change\", \"Blockchain\", \"Photosynthesis\"), use a **diagrammatic or illustration-driven** approach with non-human elements like animated symbols, natural elements, or labeled visuals.\n",
        "- In **Scene 1**, clearly establish the visual direction (character-based or illustration-based). Maintain this style consistently through all scenes.\n",
        "- Ensure a **clear narrative flow** from one scene to the next—based on:\n",
        "  - Cause and effect\n",
        "  - Escalating tension\n",
        "  - Sequential explanation\n",
        "  - Step-by-step discovery\n",
        "- Ensure that visual cues make sense for an text to image model\n",
        "\n",
        "VOICEOVER STYLE\n",
        "\n",
        "- **Narration must be in third person** only. No direct speech.\n",
        "- **Tone**: Conversational, engaging, slightly energetic—suitable for teen and young adult attention spans.\n",
        "- **Length**: Keep each narration **crisp and punchy**, like lines in a well-paced animation.\n",
        "- Describe **actions, feelings, or discoveries** to support visual storytelling.\n",
        "\n",
        "OUTPUT FORMAT\n",
        "Return only a JSON array—no extra text. Each element must follow this schema:\n",
        "\n",
        "{{\n",
        "  \"scene_number\": <integer>,\n",
        "  \"visual_cue\": \"Brief note on the setting or key visual descriptiom\",\n",
        "  \"narration\": \"Third-person description of what is happening and why it matters\"\n",
        "}}\n",
        "''')\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_msg}]\n",
        "    openai.api_key = openai_api_key\n",
        "    resp = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=1.0,\n",
        "    )\n",
        "    raw = resp.choices[0].message.content.strip()\n",
        "    try:\n",
        "\n",
        "        scenes = json.loads(raw)\n",
        "        count = len(scenes)\n",
        "        scenes = scenes[:count]\n",
        "\n",
        "        global last_scenes\n",
        "        last_scenes = scenes\n",
        "\n",
        "        print(\"Generated Scenes:\")\n",
        "        for sc in scenes:\n",
        "            print(f\"Scene {sc['scene_number']}:\")\n",
        "            print(f\"  Visual Cue: {sc['visual_cue']}\")\n",
        "            print(f\"  INarration: {sc['narration']}\\n\")\n",
        "            narrations.append(sc['narration'])\n",
        "\n",
        "        return scenes\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error parsing JSON:\", e)\n",
        "        return []\n",
        "\n",
        "# Instructions for the Stable Diffusion prompt builder.\n",
        "_SD_INSTRUCTIONS = r\"\"\"\n",
        "You are a **Stable-Diffusion prompt-builder** for **animation-style images**.\n",
        "Given an idea {{input_idea}}, return **one comma-separated keyword string**—nothing else—suitable for SD-XL **≤ 77 tokens**.\n",
        "\n",
        "─────────────────\n",
        "GLOBAL RULE — Animation Focus\n",
        "1. Every prompt must read like a frame from an animated film.\n",
        "2. Start with **one** prefix that best matches the vibe (choose automatically unless the user names one):\n",
        "   • animation still · animated frame · cartoon scene · cel-shaded concept art · hand-drawn sketch frame · stop-motion capture · claymation shot · paper-cut artwork\n",
        "3. If the user *explicitly* requests **Anime**, override with → **masterpiece, best quality, (Anime:1.4),** at the very start.\n",
        "\n",
        "─────────────────\n",
        "PROMPT-BUILDING CHECKLIST\n",
        "• Formula “[chosen animation prefix] of [main subject], [1–3 style cues]”\n",
        "• Main subject 1–2 concrete nouns + adjectives; no long verb chains.\n",
        "• Style cues concept-art tags, aesthetics (steampunk, vaporwave…), named artists (≤2), mediums (oil on canvas if you want painterly animation, etc.).\n",
        "• Composition / camera portrait, ultrawide, macro, bird’s-eye… (optional).\n",
        "• Color / lighting cinematic lighting, vivid colors, neon glow, golden hour…\n",
        "• Positive phrasing state what *is*, never what *is not*.\n",
        "• Specific counts use singular nouns or explicit numbers.\n",
        "• Distill complexity bold, readable visuals trump over-detailed ones.\n",
        "• Anything unstated is random—specify only what truly matters.\n",
        "• Add \"Vibrant colours\" keyword in the prompt.\n",
        "\n",
        "\n",
        "\n",
        "Special prefix for **photographic** requests (rare in animation workflows):\n",
        "(((photographic, photo, photogenic))), extremely high quality high detail RAW color photo,\n",
        "\n",
        "Forbidden anywhere in the output:\n",
        "category labels (Subject, Medium, Style, Artist, Website, Resolution, Additional details, Color, Lighting), articles (“a”, “the”, “there”), quote marks, phrases “the image”, “the overall tone”, “by artist”.\n",
        "\n",
        "If nudity is present, include **nude** and omit “tasteful” or “respectful”.\n",
        "\n",
        "─────────────────\n",
        "TRY LISTS (optionally pick 0–2 from each)\n",
        "• Objects / Figures wizard, angel, necromancer, city, queen, temple, farm, rockstar …\n",
        "• Feelings / Themes “sense of awe”, “birth of time”, “desire for knowledge”, “shores of infinity” …\n",
        "• Styles cyberpunk, solarpunk, surreal, vaporwave, psychedelic, minimalism, impressionism …\n",
        "• Mediums watercolor painting, charcoal sketch, woodblock print, graffiti mural, stone sculpture …\n",
        "• Artists James Gurney, MC Escher, Salvador Dali, Alphonse Mucha, Greg Rutkowski, Studio Ghibli …\n",
        "\n",
        "\n",
        "─────────────────\n",
        "EXAMPLES\n",
        "\n",
        "Input idea: **“Photosynthesis explained visually”**\n",
        "Output keywords (≤77 tokens):\n",
        "animation still of sunflower cross-section glowing chloroplasts absorbing sunlight, educational infographic style, vivid green and gold palette, warm backlight, ultra-detailed, concept art, by James Gurney, ArtStation\n",
        "\n",
        "Input idea: **“Cyberpunk shinto priest”**\n",
        "Output keywords:\n",
        "cartoon scene of cyberpunk shinto priest in neon alley holding holographic ofuda, rain-slick pavement reflection, dramatic rim light, ultra-detailed, vaporwave palette, by Greg Rutkowski and Ross Tran\n",
        "\"\"\"\n",
        "\n",
        "# Function to generate a Stable Diffusion prompt for a given scene using the GPT model.\n",
        "def sd_prompt_for_scene(scene: str) -> tuple:\n",
        "    openai.api_key = openai_api_key  # Replace with your API key\n",
        "    response = openai.ChatCompletion.create(\n",
        "    model=GPT_MODEL_2,\n",
        "    temperature=1.0,\n",
        "    messages=[\n",
        "               {\"role\": \"system\", \"content\": _SD_INSTRUCTIONS},\n",
        "               {\"role\": \"user\", \"content\": f\"Scene:\\n{scene}\"}\n",
        "           ]\n",
        "       )\n",
        "    text = response.choices[0].message.content.strip()\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "# Function to generate a list of Stable Diffusion prompts from a list of scenes.\n",
        "def generate_sd_prompts(scenes: list) -> list:\n",
        "    prompts = []\n",
        "    for i, scene in enumerate(tqdm.tqdm(scenes, desc=\"Generating SD prompts\")):\n",
        "        prompt = sd_prompt_for_scene(scene)\n",
        "        prompts.append(prompt)\n",
        "        if i < len(scenes) - 1:\n",
        "            time.sleep(5)\n",
        "    return prompts\n",
        "\n",
        "# Function to generate a storyboard from a list of prompts using Stable Diffusion models.\n",
        "def generate_storyboard(\n",
        "    prompt_list: list,\n",
        "    model_id=base_model,\n",
        "    prefix=\"high quality, detailed digital art of \",\n",
        "    strength=0.8,\n",
        "    guidance_scale=7.5,\n",
        "    use_similarity=True,\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    \"\"\"Generate a storyboard from text prompts\"\"\"\n",
        "\n",
        "    if not prompt_list:\n",
        "        return None, \"Error: No valid prompts provided\"\n",
        "    device = \"cuda\"\n",
        "    try:\n",
        "        progress(0, desc=\"Loading models...\")\n",
        "        base, refiner, bert_tokenizer, bert_model = load_global_models(model_id)\n",
        "        generated_images = []\n",
        "        for idx, prompt in enumerate(prompt_list):\n",
        "            full_prompt = prefix + prompt\n",
        "            progress_val = 0.05 + 0.9 * (idx / len(prompt_list))\n",
        "            progress(progress_val, desc=f\"Generating image {idx+1}/{len(prompt_list)}: {prompt}\")\n",
        "\n",
        "            # Step 1: Generate latent image from base model\n",
        "            base_output = base(\n",
        "                prompt=full_prompt,\n",
        "                num_inference_steps=50,\n",
        "                guidance_scale=guidance_scale,\n",
        "                output_type=\"latent\",\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            # Step 2: Refine using refiner\n",
        "            refined_output = refiner(\n",
        "                prompt=full_prompt,\n",
        "                num_inference_steps=20,\n",
        "                guidance_scale=guidance_scale,\n",
        "                image=base_output.images,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            final_image = refined_output.images[0]\n",
        "            generated_images.append(final_image)\n",
        "\n",
        "        # Create a grid of images\n",
        "        rows = (len(generated_images) + 2) // 3  # 3 images per row\n",
        "        grid_height = rows * 512\n",
        "        grid_width = min(3, len(generated_images)) * 512\n",
        "\n",
        "        grid = Image.new('RGB', (grid_width, grid_height))\n",
        "        for i, img in enumerate(generated_images):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "            grid.paste(img, (col * 512, row * 512))\n",
        "\n",
        "        # Save images and prompts to output folder\n",
        "        os.makedirs(\"output\", exist_ok=True)\n",
        "        for i, img in enumerate(generated_images):\n",
        "            img.save(f\"output/frame_{i+1}.png\")\n",
        "\n",
        "        with open(\"output/prompts.txt\", \"w\") as f:\n",
        "            for i, prompt in enumerate(prompt_list):\n",
        "                f.write(f\"Frame {i+1}: {prompt}\\n\")\n",
        "\n",
        "        with zipfile.ZipFile(\"output/storyboard.zip\", \"w\") as zipf:\n",
        "            for i in range(len(generated_images)):\n",
        "                zipf.write(f\"output/frame_{i+1}.png\", f\"frame_{i+1}.png\")\n",
        "            zipf.write(\"output/prompts.txt\", \"prompts.txt\")\n",
        "\n",
        "        return grid, \"Storyboard generated successfully! Click the download button to get all images.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error generating storyboard: {e}\"\n",
        "\n",
        "# Function to generate a storyboard and its corresponding Stable Diffusion prompts from a given topic.\n",
        "def generate_from_topic(\n",
        "    topic, model_id, prefix, strength, guidance_scale, use_similarity\n",
        "):\n",
        "    scenes = generate_storyboard_script(topic)\n",
        "    prompt_pairs = generate_sd_prompts(scenes)\n",
        "    return generate_storyboard(\n",
        "        prompt_pairs,\n",
        "        model_id=model_id,\n",
        "        prefix=prefix,\n",
        "        strength=strength,\n",
        "        guidance_scale=guidance_scale,\n",
        "        use_similarity=use_similarity\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to create an animation from generated storyboard images and narrations.\n"
      ],
      "metadata": {
        "id": "iRf7Ohn4RwoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qeUu9VQHnCCD"
      },
      "outputs": [],
      "source": [
        "def create_animation():\n",
        "    try:\n",
        "        # Check if images exist\n",
        "        if not os.path.exists(\"output/frame_1.png\"):\n",
        "            return None, \"Please generate a storyboard first\"\n",
        "        frame_files = [\n",
        "            f for f in os.listdir(\"output\")\n",
        "            if f.startswith(\"frame_\") and f.endswith(\".png\")\n",
        "        ]\n",
        "\n",
        "        frame_files.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
        "\n",
        "        # Read frames and prompts\n",
        "        images = [Image.open(f\"output/{f}\") for f in frame_files]\n",
        "        min_len = len(images)\n",
        "        images = images[:min_len]\n",
        "\n",
        "        video_path = create_storyboard_animation(images=images,prompts=narrations)\n",
        "\n",
        "        return video_path, \"Animation created successfully! Click the download button to get the video.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error creating animation: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create Gradio Interface\n",
        "# Defines the Gradio interface for the application."
      ],
      "metadata": {
        "id": "6jMplHOrSFvL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZEPo_C_PkXu",
        "outputId": "680340a8-471f-4d8b-ea28-2249e3f69669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3398649297fc5445dc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3398649297fc5445dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_css = \"\"\"\n",
        "body {\n",
        "    background-color: #000814 !important;\n",
        "    color: #00f0ff !important;\n",
        "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "}\n",
        "\n",
        ".gradio-container, .gr-block, .gr-panel, .gr-box {\n",
        "    background-color: #000814 !important;\n",
        "    color: #00f0ff !important;\n",
        "    border: none !important;\n",
        "}\n",
        "\n",
        "h1, h2, h3,\n",
        ".gr-markdown h1, .gr-markdown h2, .gr-markdown h3 {\n",
        "    color: #00f0ff !important;\n",
        "    text-shadow: 0 0 6px rgba(0, 240, 255, 0.6);\n",
        "}\n",
        "\n",
        "input, textarea, select {\n",
        "    background-color: #001d2e !important;\n",
        "    color: #00f0ff !important;\n",
        "    border: 1px solid #00f0ff !important;\n",
        "    box-shadow: 0 0 10px rgba(0, 240, 255, 0.3) inset !important;\n",
        "    border-radius: 6px;\n",
        "}\n",
        "\n",
        "button, .gr-button {\n",
        "    background-color: #001d2e !important;\n",
        "    color: #00f0ff !important;\n",
        "    border: 1px solid #00f0ff !important;\n",
        "    box-shadow: 0 0 12px rgba(0, 240, 255, 0.5);\n",
        "    border-radius: 6px;\n",
        "    transition: background-color 0.3s;\n",
        "}\n",
        "button:hover, .gr-button:hover {\n",
        "    background-color: #003344 !important;\n",
        "    box-shadow: 0 0 16px rgba(0, 240, 255, 0.75);\n",
        "}\n",
        "\n",
        ".gr-slider .noUi-target {\n",
        "    background: #001a26 !important;\n",
        "    border: 1px solid #00f0ff !important;\n",
        "}\n",
        ".gr-slider .noUi-connect {\n",
        "    background: #00f0ff !important;\n",
        "}\n",
        ".gr-slider .noUi-handle {\n",
        "    background: #003344 !important;\n",
        "    border: 2px solid #00f0ff !important;\n",
        "    box-shadow: 0 0 10px rgba(0, 240, 255, 0.6);\n",
        "}\n",
        "\n",
        ".gr-tabs, .gr-tabitem {\n",
        "    background-color: #00121f !important;\n",
        "    color: #00f0ff !important;\n",
        "}\n",
        ".gr-tabs .tabitem.selected {\n",
        "    background-color: #003344 !important;\n",
        "    border-bottom: 2px solid #00f0ff !important;\n",
        "}\n",
        "\n",
        ".gr-image img, .gr-video video {\n",
        "    border: 1px solid #00f0ff !important;\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 0 16px rgba(0, 240, 255, 0.3);\n",
        "}\n",
        "\n",
        "input[type=\"checkbox\"] {\n",
        "    accent-color: #00f0ff !important;\n",
        "}\n",
        ".gr-checkbox label {\n",
        "    color: #00f0ff !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Add css=custom_css to your interface:\n",
        "with gr.Blocks(title=\"Stable Diffusion Storyboard Generator\", css=custom_css) as demo:\n",
        "    gr.Markdown(\"# 🎬 Stable Diffusion Storyboard Generator\")\n",
        "    gr.Markdown(\"Generate a sequence of images for a visual storyboard from text prompts\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Generate Storyboard\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    topic = gr.Textbox(\n",
        "                        label=\"Storyboard Topic (one line)\",\n",
        "                        placeholder=\"e.g. Photosynthesis process in a leaf…\"\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        model_id = gr.Dropdown(\n",
        "                            label=\"Model\",\n",
        "                            choices=[base_model],\n",
        "                            value=base_model\n",
        "                        )\n",
        "                        prefix = gr.Textbox(\n",
        "                            label=\"Style Prefix\",\n",
        "                            value=\"high quality, detailed digital art of \"\n",
        "                        )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        strength = gr.Slider(\n",
        "                            label=\"Transformation Strength\",\n",
        "                            minimum=0.1,\n",
        "                            maximum=1.0,\n",
        "                            value=0.8,\n",
        "                            step=0.05\n",
        "                        )\n",
        "                        guidance_scale = gr.Slider(\n",
        "                            label=\"Guidance Scale\",\n",
        "                            minimum=1.0,\n",
        "                            maximum=20.0,\n",
        "                            value=7.5,\n",
        "                            step=0.5\n",
        "                        )\n",
        "\n",
        "                    use_similarity = gr.Checkbox(\n",
        "                        label=\"Use similarity for continuity\",\n",
        "                        value=True\n",
        "                    )\n",
        "\n",
        "                    generate_btn = gr.Button(\"🚀 Generate Storyboard\", variant=\"primary\")\n",
        "                    download_btn = gr.File(label=\"Download ZIP with all images\", interactive=False)\n",
        "                    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    gallery = gr.Image(label=\"Generated Storyboard\", interactive=False)\n",
        "                    Feedback = gr.Textbox(\n",
        "                        label=\"Feedback\",\n",
        "                        placeholder=\"Provide Feedback for the images generated.\",\n",
        "                        lines=10\n",
        "                    )\n",
        "\n",
        "            generate_btn.click(\n",
        "                fn=generate_from_topic,\n",
        "                inputs=[\n",
        "                    topic,\n",
        "                    model_id,\n",
        "                    prefix,\n",
        "                    strength,\n",
        "                    guidance_scale,\n",
        "                    use_similarity\n",
        "                ],\n",
        "                outputs=[gallery, status]\n",
        "            ).then(\n",
        "                fn=create_download_btn,\n",
        "                inputs=None,\n",
        "                outputs=download_btn\n",
        "            )\n",
        "\n",
        "        with gr.TabItem(\"Create Animation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    animate_btn = gr.Button(\"🎞️ Create Animation\", variant=\"primary\")\n",
        "                    video_status = gr.Textbox(label=\"Animation Status\", interactive=False)\n",
        "                    video_download = gr.File(label=\"Download Animation\", interactive=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    video_output = gr.Video(label=\"Preview Animation\")\n",
        "\n",
        "            animate_btn.click(\n",
        "                fn=create_animation,\n",
        "                inputs=[],\n",
        "                outputs=[video_output, video_status]\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## How to use\n",
        "    1. Enter your storyboard prompts (one per line)\n",
        "    2. Adjust generation parameters as needed\n",
        "    3. Click \"Generate Storyboard\" button\n",
        "    4. Download the ZIP file containing all images\n",
        "\n",
        "    ## Parameters\n",
        "    - **Style Prefix**: Text added to the beginning of each prompt\n",
        "    - **Transformation Strength**: How much to transform the previous image (0.1-1.0)\n",
        "    - **Guidance Scale**: How closely to follow the prompt (higher = more faithful)\n",
        "    - **Use similarity for continuity**: Uses the most similar previous frame instead of just the last one\n",
        "    \"\"\")\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}